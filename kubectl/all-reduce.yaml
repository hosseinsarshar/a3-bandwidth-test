apiVersion: v1
kind: Service
metadata:
  name:  nccl-host-1
spec:
  selector:
    name:  nccl-host-1
  clusterIP: None
---

apiVersion: v1
kind: Service
metadata:
  name: nccl-host-2
spec:
  selector:
    name: nccl-host-2
  clusterIP: None
---
apiVersion: v1
kind: Pod
metadata:
  name: nccl-test-host-1
  labels:
    name: nccl-host-1
spec:
  hostname: host1
  subdomain: nccl-host-1
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  containers:
    - name: tcpxo-daemon
      image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.8
      imagePullPolicy: Always
      command: ["/bin/sh", "-c"]
      args:
        - |
          set -ex
          chmod 755 /fts/entrypoint_rxdm_container.sh
          /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
      securityContext:
        privileged: true
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
      env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
    - name: nccl-test
      image: nvcr.io/nvidia/pytorch:24.05-py3
      imagePullPolicy: Always
      command: ["sh", "-c"] # Use sh to execute multiple commands
      args:
        - |
          python -c "print('******* All Reduce Network Benchmark Starts *******')"
          rm -f /usr/share/all_reduce_benchmarks/workload_terminated

          python -c "print('Number of nodes participating: 2')"
          echo NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS: $NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS
          echo MASTER_ADDR: $MASTER_ADDR
          git clone https://github.com/hosseinsarshar/ml-engineering.git
          python -u -m torch.distributed.run \
            --nproc_per_node 8 \
            --nnodes 2 \
            --rdzv_endpoint nccl-host-1:6000 \
            --rdzv_backend c10d \
            --max_restarts 0 \
            --role `hostname -s`: \
            --tee 3 \
            ml-engineering/network/benchmarks/all_reduce_bench.py
          python -c "print('******* All Reduce Network Benchmark Completed *******')"
          while true; do echo 'Running'; sleep 60; done
      env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        - name: MASTER_ADDR
          value: "nccl-benchmarks-{{ $.Release.Name }}-{{ $timestamp }}"
        - name: NNODES
          value: "{{ $node_count }}"
        - name: NODE_RANK
          value: "{{ $node_index }}"
        - name: GCS_BUCKET
          value: "{{ $.Values.cluster.gcsBucket }}"
        - name: LD_LIBRARY_PATH
          value: "/usr/local/nvidia/lib64"
        - name: BENCHMARKS_CSV
          value: "{{ $.Values.ncclBenchmarks.benchmarks }}"
        - name: MASKS_CSV
          value: "{{ $.Values.ncclBenchmarks.masks }}"
        - name: MSG_SIZE_BEGIN
          value: "{{ $.Values.ncclBenchmarks.msgSizeBegin }}"
        - name: MSG_SIZE_END
          value: "{{ $.Values.ncclBenchmarks.msgSizeEnd }}"
        - name: GPUS_PER_NODE
          value: "{{ $.Values.ncclBenchmarks.gpusPerNode }}"
        - name: WARMUP_ITERS
          value: "{{ $.Values.ncclBenchmarks.warmupIters }}"
        - name: RUN_ITERS
          value: "{{ $.Values.ncclBenchmarks.runIters }}"
        - name: N_RUNS
          value: "{{ $.Values.ncclBenchmarks.nRuns }}"
        - name: UNRESERVED_CORES
          value: "{{ $.Values.ncclPlugin.unreservedCores }}"
        - name: GPU_TELEMETRY
          value: "{{ $.Values.telemetry.gpu }}"
        - name: NCCL_LIB_DIR
          value: "/usr/local/nvidia/lib64"
        - name: NCCL_FASTRAK_IFNAME
          value: "eth1,eth2,eth3,eth4,eth5,eth6,eth7,eth8"
        - name: NCCL_FASTRAK_CTRL_DEV
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_CROSS_NIC
          value: "0"
        - name: NCCL_ALGO
          value: "Ring,Tree"
        - name: NCCL_PROTO
          value: "Simple"
        - name: NCCL_MIN_NCHANNELS
          value: "4"
        - name: NCCL_P2P_NET_CHUNKSIZE
          value: "524288"
        - name: NCCL_P2P_PCI_CHUNKSIZE
          value: "524288"
        - name: NCCL_P2P_NVL_CHUNKSIZE
          value: "1048576"
        - name: NCCL_FASTRAK_NUM_FLOWS
          value: "2"
        - name: NCCL_FASTRAK_ENABLE_CONTROL_CHANNEL
          value: "0"
        - name: NCCL_BUFFSIZE
          value: "8388608"
        - name: NCCL_FASTRAK_USE_SNAP
          value: "1"
        - name: NCCL_FASTRAK_USE_LLCM
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3,4,5,6,7"
        - name: NCCL_NET_GDR_LEVEL
          value: "PIX"
        - name: NCCL_FASTRAK_ENABLE_HOTPATH_LOGGING
          value: "0"
        - name: NCCL_TUNER_PLUGIN
          value: "libnccl-tuner.so"
        - name: NCCL_TUNER_CONFIG_PATH
          value: "${NCCL_LIB_DIR}/a3plus_tuner_config.textproto"
        - name: NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE
          value: "${NCCL_LIB_DIR}/a3plus_guest_config.textproto"
        - name: NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS
          value: "600000"
        - name: NCCL_NVLS_ENABLE
          value: "0"
      securityContext:
        privileged: true
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
        - name: shared-memory
          mountPath: /dev/shm
      resources:
        limits:
          nvidia.com/gpu: 8
  volumes:
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 1Gi

---
apiVersion: v1
kind: Pod
metadata:
  name: nccl-test-host-2
  labels:
    name: nccl-host-2
spec:
  hostname: host2
  subdomain: nccl-host-2
  hostNetwork: true
  dnsPolicy: ClusterFirstWithHostNet
  containers:
    - name: tcpxo-daemon
      image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpxo/tcpgpudmarxd-dev:v1.0.8
      imagePullPolicy: Always
      command: ["/bin/sh", "-c"]
      args:
        - |
          set -ex
          chmod 755 /fts/entrypoint_rxdm_container.sh
          /fts/entrypoint_rxdm_container.sh --num_hops=2 --num_nics=8 --uid= --alsologtostderr
      securityContext:
        privileged: true
      volumeMounts:
        - name: nvidia-install-dir-host
          mountPath: /usr/local/nvidia
      env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
    - name: nccl-test
      image: nvcr.io/nvidia/pytorch:24.05-py3
      imagePullPolicy: Always
      command: ["sh", "-c"] # Use sh to execute multiple commands
      args:
        - |
          python -c "print('******* All Reduce Network Benchmark Starts *******')"
          rm -f /usr/share/all_reduce_benchmarks/workload_terminated

          python -c "print('Number of nodes participating: 2')"
          echo NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS: $NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS
          echo MASTER_ADDR: $MASTER_ADDR
          git clone https://github.com/hosseinsarshar/ml-engineering.git
          python -u -m torch.distributed.run \
            --nproc_per_node 8 \
            --nnodes 2 \
            --rdzv_endpoint nccl-host-1:6000 \
            --rdzv_backend c10d \
            --max_restarts 0 \
            --role `hostname -s`: \
            --tee 3 \
            ml-engineering/network/benchmarks/all_reduce_bench.py
          python -c "print('******* All Reduce Network Benchmark Completed *******')"
          while true; do echo 'Running'; sleep 60; done
      env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        - name: MASTER_ADDR
          value: "nccl-benchmarks-{{ $.Release.Name }}-{{ $timestamp }}"
        - name: NNODES
          value: "{{ $node_count }}"
        - name: NODE_RANK
          value: "{{ $node_index }}"
        - name: GCS_BUCKET
          value: "{{ $.Values.cluster.gcsBucket }}"
        - name: LD_LIBRARY_PATH
          value: "/usr/local/nvidia/lib64"
        - name: BENCHMARKS_CSV
          value: "{{ $.Values.ncclBenchmarks.benchmarks }}"
        - name: MASKS_CSV
          value: "{{ $.Values.ncclBenchmarks.masks }}"
        - name: MSG_SIZE_BEGIN
          value: "{{ $.Values.ncclBenchmarks.msgSizeBegin }}"
        - name: MSG_SIZE_END
          value: "{{ $.Values.ncclBenchmarks.msgSizeEnd }}"
        - name: GPUS_PER_NODE
          value: "{{ $.Values.ncclBenchmarks.gpusPerNode }}"
        - name: WARMUP_ITERS
          value: "{{ $.Values.ncclBenchmarks.warmupIters }}"
        - name: RUN_ITERS
          value: "{{ $.Values.ncclBenchmarks.runIters }}"
        - name: N_RUNS
          value: "{{ $.Values.ncclBenchmarks.nRuns }}"
        - name: UNRESERVED_CORES
          value: "{{ $.Values.ncclPlugin.unreservedCores }}"
        - name: GPU_TELEMETRY
          value: "{{ $.Values.telemetry.gpu }}"
        - name: NCCL_LIB_DIR
          value: "/usr/local/nvidia/lib64"
        - name: NCCL_FASTRAK_IFNAME
          value: "eth1,eth2,eth3,eth4,eth5,eth6,eth7,eth8"
        - name: NCCL_FASTRAK_CTRL_DEV
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_CROSS_NIC
          value: "0"
        - name: NCCL_ALGO
          value: "Ring,Tree"
        - name: NCCL_PROTO
          value: "Simple"
        - name: NCCL_MIN_NCHANNELS
          value: "4"
        - name: NCCL_P2P_NET_CHUNKSIZE
          value: "524288"
        - name: NCCL_P2P_PCI_CHUNKSIZE
          value: "524288"
        - name: NCCL_P2P_NVL_CHUNKSIZE
          value: "1048576"
        - name: NCCL_FASTRAK_NUM_FLOWS
          value: "2"
        - name: NCCL_FASTRAK_ENABLE_CONTROL_CHANNEL
          value: "0"
        - name: NCCL_BUFFSIZE
          value: "8388608"
        - name: NCCL_FASTRAK_USE_SNAP
          value: "1"
        - name: NCCL_FASTRAK_USE_LLCM
          value: "1"
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3,4,5,6,7"
        - name: NCCL_NET_GDR_LEVEL
          value: "PIX"
        - name: NCCL_FASTRAK_ENABLE_HOTPATH_LOGGING
          value: "0"
        - name: NCCL_TUNER_PLUGIN
          value: "libnccl-tuner.so"
        - name: NCCL_TUNER_CONFIG_PATH
          value: "${NCCL_LIB_DIR}/a3plus_tuner_config.textproto"
        - name: NCCL_SHIMNET_GUEST_CONFIG_CHECKER_CONFIG_FILE
          value: "${NCCL_LIB_DIR}/a3plus_guest_config.textproto"
        - name: NCCL_FASTRAK_PLUGIN_ACCEPT_TIMEOUT_MS
          value: "600000"
        - name: NCCL_NVLS_ENABLE
          value: "0"
      securityContext:
        privileged: true
      volumeMounts:
        - name: shared-memory
          mountPath: /dev/shm
      resources:
        limits:
          nvidia.com/gpu: 8
  volumes:
    - name: nvidia-install-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 1Gi